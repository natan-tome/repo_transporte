<br>
<br>

# ğŸš€ Projeto Checkpoint 02 â€“ Pipeline de Infraestrutura e IngestÃ£o de Dados

<br>

## ğŸ“– SumÃ¡rio

- [DescriÃ§Ã£o do Projeto e SoluÃ§Ã£o](#descriÃ§Ã£o-do-projeto-e-soluÃ§Ã£o)
- [Arquitetura Geral e Fluxo de Dados](#arquitetura-geral-e-fluxo-de-dados)
- [Estrutura do RepositÃ³rio](#estrutura-do-repositÃ³rio)
- [Fontes de Dados, Formatos e Schemas](#fontes-de-dados-formatos-e-schemas)
- [Tecnologias, Ferramentas e DependÃªncias](#tecnologias-ferramentas-e-dependÃªncias)
    - [Por Container/Ferramenta](#por-containerferramenta)
- [ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento](#configuraÃ§Ã£o-do-ambiente-de-desenvolvimento)
- [Passo a Passo de ExecuÃ§Ã£o (Guia do UsuÃ¡rio)](#passo-a-passo-de-execuÃ§Ã£o-guia-do-usuÃ¡rio)
- [Manual de Uso do Makefile](#manual-de-uso-do-makefile)
- [MÃ©todos de Acesso e AutenticaÃ§Ã£o](#mÃ©todos-de-acesso-e-autenticaÃ§Ã£o)
- [EstratÃ©gia de ExtraÃ§Ã£o e Mapeamento dos Dados](#estratÃ©gia-de-extraÃ§Ã£o-e-mapeamento-dos-dados)
- [DecisÃµes TÃ©cnicas e Justificativas](#decisÃµes-tÃ©cnicas-e-justificativas)
- [Escalabilidade, OtimizaÃ§Ã£o, Modularidade e Reusabilidade](#escalabilidade-otimizaÃ§Ã£o-modularidade-e-reusabilidade)
- [DescriÃ§Ã£o dos Recursos de Infraestrutura](#descriÃ§Ã£o-dos-recursos-de-infraestrutura)

<br>

## ğŸ“‹ DescriÃ§Ã£o do Projeto e SoluÃ§Ã£o

Este projeto entrega um pipeline de dados robusto, modular e escalÃ¡vel, capaz de extrair dados de um banco relacional SQL Server e de uma API REST, transformar e carregar estes dados no Databricks Delta Lake, utilizando boas prÃ¡ticas de engenharia de dados, conteinerizaÃ§Ã£o e versionamento seguro. Toda a orquestraÃ§Ã£o Ã© realizada via Makefile, garantindo reprodutibilidade e facilidade operacional.

<br>

## ğŸ—ï¸ Arquitetura Geral e Fluxo de Dados

1. **ExtraÃ§Ã£o SQL Server (Embulk):** Dados extraÃ­dos do banco relacional e convertidos em Parquet.
2. **ExtraÃ§Ã£o API (Meltano):** Dados extraÃ­dos da API REST e salvos em JSONL.
3. **IngestÃ£o Databricks:** Arquivos Parquet e JSONL sÃ£o enviados para volumes no Databricks.
4. **Provisionamento Infraestrutura (Terraform):** CriaÃ§Ã£o de volumes, schemas e notebooks no Databricks.
5. **Processamento Delta (Notebook):** ConversÃ£o dos arquivos para tabelas Delta Lake.
6. **OrquestraÃ§Ã£o:** Todo o fluxo Ã© automatizado via Makefile e Docker Compose.

<br>

## ğŸ“‚ Estrutura do RepositÃ³rio
```bash
.
â”œâ”€â”€ ap_extracao_embulk/ # ğŸ­ Pipeline Embulk (SQL Server â†’ Parquet)
â”‚ â”œâ”€â”€ config/
â”‚ â”‚ â”œâ”€â”€ template.yml
â”‚ â”‚ â””â”€â”€ extracao_tabelas.yml
â”‚ â”œâ”€â”€ scripts/
â”‚ â”‚ â””â”€â”€ criacao_yml.sh
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ ap_extracao_meltano/ # ğŸŒ Pipeline Meltano (API â†’ JSONL)
â”‚ â”œâ”€â”€ meltano.yml
â”‚ â”œâ”€â”€ plugins/
â”‚ â”‚ â””â”€â”€ custom/tap-aw-api/
â”‚ â”‚ â”œâ”€â”€ tap.py, client.py, streams.py, ...
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ ap_infra_pipeline/ # ğŸ—ï¸ Infraestrutura Databricks (Terraform)
â”‚ â”œâ”€â”€ main.tf, variables.tf, ...
â”‚ â”œâ”€â”€ notebooks/
â”‚ â”‚ â””â”€â”€ create_delta_tables.py
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â”œâ”€â”€ destroy.sh
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ ap_ingestao_databricks/ # â¬†ï¸ Upload de arquivos para Databricks
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ ap_processamento_job/ # âš¡ ExecuÃ§Ã£o de jobs Databricks
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â”œâ”€â”€ destroy.sh
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml # ğŸ³ OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ Makefile # ğŸ› ï¸ OrquestraÃ§Ã£o dos comandos do pipeline
â”œâ”€â”€ .env # ğŸ”’ VariÃ¡veis de ambiente (Template para preenchimento)
â””â”€â”€ README.md # ğŸ“– Este arquivo
```

<br>

## ğŸ—„ï¸ Fontes de Dados, Formatos e Schemas

- **Banco Relacional (SQL Server):**  
  - Tabelas: [Ver lista completa em `extracao_tabelas.yml`][1]
  - Formato de saÃ­da: Parquet
- **API REST (AdventureWorks):**  
  - Endpoints: PurchaseOrderDetail, PurchaseOrderHeader, SalesOrderDetail, SalesOrderHeader
  - Formato de saÃ­da: JSONL
- **Schema dos Dados:**  
  - Definido nos arquivos JSON de schema e YAML do projeto.
  - Mapeamento automÃ¡tico via Embulk e Meltano.

<br>

## ğŸ› ï¸ Tecnologias, Ferramentas e DependÃªncias

### Por Container/Ferramenta

#### **1. Embulk (ap_extracao_embulk)**
- **Tecnologia:** Embulk
- **DependÃªncias:** JDK, JRuby, Parquet plugin, yq
- **Bibliotecas:** embulk-input-sqlserver, embulk-output-parquet
- **DescriÃ§Ã£o:** Ferramenta robusta para extraÃ§Ã£o de dados de SQL Server e conversÃ£o para Parquet. Utiliza templates YAML para geraÃ§Ã£o dinÃ¢mica dos jobs de extraÃ§Ã£o.  
- **BenefÃ­cios:** Alta performance, fÃ¡cil integraÃ§Ã£o, automaÃ§Ã£o de jobs via shell script.

#### **2. Meltano (ap_extracao_meltano)**
- **Tecnologia:** Meltano, Singer SDK, Python 3.10
- **DependÃªncias:** Python, pip, Singer SDK, requests
- **Bibliotecas:** tap-aw-api (custom), target-jsonl
- **DescriÃ§Ã£o:** Plataforma de ELT moderna, utilizada para extrair dados de APIs REST e salvar em JSONL. Tap customizado para AdventureWorks API.
- **BenefÃ­cios:** Modularidade, extensibilidade, fÃ¡cil integraÃ§Ã£o com pipelines modernos.

#### **3. Infraestrutura Databricks (ap_infra_pipeline)**
- **Tecnologia:** Terraform, Databricks CLI, Python 3
- **DependÃªncias:** terraform, python3, pip, databricks-cli, requests
- **Bibliotecas:** databricks-cli, requests
- **DescriÃ§Ã£o:** Provisionamento de volumes, schemas, notebooks e jobs no Databricks via cÃ³digo declarativo.
- **BenefÃ­cios:** Infraestrutura como cÃ³digo, reprodutibilidade, versionamento seguro.

#### **4. IngestÃ£o Databricks (ap_ingestao_databricks)**
- **Tecnologia:** Databricks CLI, Bash
- **DependÃªncias:** databricks-cli, jq, bash
- **DescriÃ§Ã£o:** Upload automatizado dos arquivos Parquet e JSONL para os volumes do Databricks.
- **BenefÃ­cios:** AutomaÃ§Ã£o, integraÃ§Ã£o transparente com o pipeline.

#### **5. Processamento Job (ap_processamento_job)**
- **Tecnologia:** Databricks CLI, Bash
- **DependÃªncias:** databricks-cli, bash
- **DescriÃ§Ã£o:** ExecuÃ§Ã£o de jobs Databricks que convertem arquivos Parquet/JSONL em tabelas Delta.
- **BenefÃ­cios:** OrquestraÃ§Ã£o automatizada, integraÃ§Ã£o com variÃ¡veis de ambiente.

<br>

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento

1. **Clone o repositÃ³rio:**

git clone <URL_DO_SEU_REPOSITORIO>
cd <nome_do_repositorio>


2. **Configure o arquivo `.env`:**
- Preencha todas as variÃ¡veis do template `.env` com suas credenciais e informaÃ§Ãµes de ambiente.
- **Nunca** commite o arquivo `.env` preenchido.

3. **Instale o Docker e Docker Compose** (caso ainda nÃ£o tenha):
- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)

<br>

## ğŸ§‘â€ğŸ’» Passo a Passo de ExecuÃ§Ã£o (Guia do UsuÃ¡rio)

1. **Edite o arquivo `.env` com suas credenciais:**
- Insira dados de acesso ao SQL Server, API, Databricks e demais variÃ¡veis obrigatÃ³rias.
- Exemplo de variÃ¡veis:
  ```
  SQLSERVER_HOST=
  SQLSERVER_PORT=
  SQLSERVER_USER=
  SQLSERVER_PASSWORD=
  SQLSERVER_DB=
  API_USERNAME=
  API_PASSWORD=
  API_URL=
  API_START_DATE=
  DATABRICKS_HOST=
  DATABRICKS_TOKEN=
  CATALOG_NAME=
  SCHEMA_NAME=
  EMAIL=
  ```

2. **Suba os containers e execute o pipeline completo:**

make all


3. **Manual de uso do Makefile:**

| Comando            | DescriÃ§Ã£o                                                                 |
|--------------------|---------------------------------------------------------------------------|
| `make init`        | ğŸ³ Inicializa todos os containers (build/rebuild)                          |
| `make extract_api` | ğŸŒ Executa extraÃ§Ã£o de dados via API (Meltano)                             |
| `make extract_db`  | ğŸ­ Executa extraÃ§Ã£o de dados do banco relacional via Embulk                |
| `make provision_infra` | ğŸ—ï¸ Provisiona recursos no Databricks via Terraform                   |
| `make load_databricks` | â¬†ï¸ Carrega arquivos para o Databricks                                 |
| `make run_job`     | âš¡ Executa o job Databricks para criar tabelas Delta                       |
| `make destroy_infra` | ğŸ’£ Destroi infraestrutura provisionada no Databricks                    |
| `make destroy_job` | ğŸ’¥ Destroi o job programado no Databricks                                  |
| `make destroy_all` | ğŸ§¹ Atalho para destruir infraestrutura e job no Databricks                 |
| `make clean`       | ğŸ—‘ï¸ Remove containers, volumes e redes Ã³rfÃ£s                               |

4. **Para executar etapas especÃ­ficas:**

make extract_db  | 
make provision_infra  | 
make load_databricks  | 
make run_job


<br>

## ğŸ”’ MÃ©todos de Acesso e AutenticaÃ§Ã£o

- **Banco de Dados:** AutenticaÃ§Ã£o via usuÃ¡rio/senha definidos no `.env`.
- **API:** AutenticaÃ§Ã£o bÃ¡sica HTTP (usuÃ¡rio/senha do `.env`).
- **Databricks:** AutenticaÃ§Ã£o via token pessoal (`DATABRICKS_TOKEN`), nunca exposto no repositÃ³rio.
- **VariÃ¡veis de ambiente:** Carregadas dinamicamente por cada container, nunca versionadas.

<br>

## ğŸ—ºï¸ EstratÃ©gia para ExtraÃ§Ã£o e Mapeamento dos Dados

- **SQL Server â†’ Parquet:** ExtraÃ§Ã£o via Embulk, jobs YAML dinÃ¢micos, schemas automatizados.
- **API â†’ JSONL:** ExtraÃ§Ã£o via Meltano, tap customizado, schemas JSON centralizados.
- **Parquet/JSONL â†’ Databricks:** Upload automatizado para volumes.
- **Volumes â†’ Delta Lake:** ConversÃ£o via notebook Python parametrizado, execuÃ§Ã£o automatizada por job Databricks.

<br>

## ğŸ§  DecisÃµes TÃ©cnicas e Justificativas

- **ConteinerizaÃ§Ã£o:** Garantia de ambiente isolado, reprodutÃ­vel e fÃ¡cil de distribuir.
- **Infraestrutura como CÃ³digo (Terraform):** Provisionamento seguro, versionado e auditÃ¡vel.
- **Makefile:** OrquestraÃ§Ã£o simples, transparente e padronizada.
- **SeparaÃ§Ã£o por mÃ³dulos:** Cada etapa do pipeline em seu prÃ³prio container/ferramenta.
- **Uso de Parquet/JSONL:** Formatos eficientes e compatÃ­veis com o Databricks.
- **SeguranÃ§a:** VariÃ¡veis sensÃ­veis nunca sÃ£o versionadas.

<br>

## ğŸ“ˆ Escalabilidade, OtimizaÃ§Ã£o, Modularidade e Reusabilidade

- **Escalabilidade:** Pipeline projetado para suportar aumento de volume de dados e novas fontes.
- **OtimizaÃ§Ã£o:** Uso de formatos colunares, paralelismo nas extraÃ§Ãµes, jobs idempotentes.
- **Modularidade:** Cada etapa isolada, facilitando manutenÃ§Ã£o e evoluÃ§Ã£o.
- **Reusabilidade:** Ferramentas e scripts podem ser reaproveitados em outros projetos.

<br>

## â˜ï¸ DescriÃ§Ã£o dos Recursos de Infraestrutura

- **Containers Docker:** Cada etapa do pipeline roda em seu prÃ³prio container.
- **Volumes Docker:** Compartilhamento eficiente de dados intermediÃ¡rios.
- **Databricks:** Armazenamento final dos dados, processamento Delta Lake, notebooks automatizados.
- **Terraform:** Gerenciamento de recursos Databricks (volumes, schemas, jobs).

<br>

## ğŸ“ ConsideraÃ§Ãµes Finais

- **Pipeline seguro, modular, escalÃ¡vel e eficiente.**
- **DocumentaÃ§Ã£o clara e detalhada para facilitar onboarding e manutenÃ§Ã£o.**
- **Pronto para demonstraÃ§Ã£o ao vivo e futuras evoluÃ§Ãµes.**

<br>

> **AtenÃ§Ã£o:**  
> - Nunca compartilhe o arquivo `.env` preenchido publicamente.  
> - Sempre valide as variÃ¡veis antes de executar o pipeline.
> - Consulte a documentaÃ§Ã£o inline dos scripts e notebooks para detalhes de uso e troubleshooting.

---
<br>


## **Boa utilizaÃ§Ã£o e boas anÃ¡lises!** ğŸš€

<br>



*[1]: extracao_tabelas.yml